#!/usr/bin/env python3
"""
APIé…ç½®æµ‹è¯•è„šæœ¬
ç”¨äºéªŒè¯config2.yamlä¸­çš„APIé…ç½®æ˜¯å¦å¯ç”¨
"""

import os
import yaml
import asyncio
from openai import AsyncOpenAI
from pathlib import Path

def load_config():
    """åŠ è½½é…ç½®æ–‡ä»¶"""
    config_path = Path(__file__).parent / "config" / "config2.yaml"
    with open(config_path, 'r', encoding='utf-8') as f:
        config = yaml.safe_load(f)
    return config['llm']

async def test_api_with_proxy(llm_config):
    """ä½¿ç”¨ä»£ç†æµ‹è¯•API"""
    print("\n" + "="*60)
    print("æµ‹è¯•1: ä½¿ç”¨ä»£ç† (127.0.0.1:7890)")
    print("="*60)
    
    # è®¾ç½®ä»£ç†
    os.environ['HTTP_PROXY'] = '127.0.0.1:7890'
    os.environ['HTTPS_PROXY'] = '127.0.0.1:7890'
    
    try:
        client = AsyncOpenAI(
            api_key=llm_config['api_key'],
            base_url=llm_config['base_url']
        )
        
        print(f"ğŸ“¡ APIç±»å‹: {llm_config['api_type']}")
        print(f"ğŸ¤– æ¨¡å‹: {llm_config['model']}")
        print(f"ğŸ”— Base URL: {llm_config['base_url']}")
        print(f"ğŸ”‘ API Key: {llm_config['api_key'][:20]}...")
        print(f"ğŸŒ ä»£ç†: 127.0.0.1:7890")
        print("\næ­£åœ¨å‘é€æµ‹è¯•è¯·æ±‚...")
        
        response = await client.chat.completions.create(
            model=llm_config['model'],
            messages=[
                {"role": "user", "content": "è¯·ç”¨ä¸€å¥è¯ä»‹ç»ä½ è‡ªå·±"}
            ],
            timeout=30.0
        )
        
        print("\nâœ… ä½¿ç”¨ä»£ç†è¿æ¥æˆåŠŸï¼")
        print(f"ğŸ“ å“åº”å†…å®¹: {response.choices[0].message.content}")
        return True
        
    except Exception as e:
        print(f"\nâŒ ä½¿ç”¨ä»£ç†è¿æ¥å¤±è´¥")
        print(f"é”™è¯¯ç±»å‹: {type(e).__name__}")
        print(f"é”™è¯¯ä¿¡æ¯: {str(e)}")
        return False
    finally:
        # æ¸…ç†ä»£ç†ç¯å¢ƒå˜é‡
        os.environ.pop('HTTP_PROXY', None)
        os.environ.pop('HTTPS_PROXY', None)

async def test_api_without_proxy(llm_config):
    """ä¸ä½¿ç”¨ä»£ç†æµ‹è¯•API"""
    print("\n" + "="*60)
    print("æµ‹è¯•2: ä¸ä½¿ç”¨ä»£ç† (ç›´è¿)")
    print("="*60)
    
    # ç¡®ä¿æ²¡æœ‰ä»£ç†è®¾ç½®
    os.environ.pop('HTTP_PROXY', None)
    os.environ.pop('HTTPS_PROXY', None)
    os.environ.pop('http_proxy', None)
    os.environ.pop('https_proxy', None)
    
    try:
        client = AsyncOpenAI(
            api_key=llm_config['api_key'],
            base_url=llm_config['base_url']
        )
        
        print(f"ğŸ“¡ APIç±»å‹: {llm_config['api_type']}")
        print(f"ğŸ¤– æ¨¡å‹: {llm_config['model']}")
        print(f"ğŸ”— Base URL: {llm_config['base_url']}")
        print(f"ğŸ”‘ API Key: {llm_config['api_key'][:20]}...")
        print(f"ğŸŒ ä»£ç†: æ—  (ç›´è¿)")
        print("\næ­£åœ¨å‘é€æµ‹è¯•è¯·æ±‚...")
        
        response = await client.chat.completions.create(
            model=llm_config['model'],
            messages=[
                {"role": "user", "content": "è¯·ç”¨ä¸€å¥è¯ä»‹ç»ä½ è‡ªå·±"}
            ],
            timeout=30.0
        )
        
        print("\nâœ… ç›´è¿æˆåŠŸï¼")
        print(f"ğŸ“ å“åº”å†…å®¹: {response.choices[0].message.content}")
        return True
        
    except Exception as e:
        print(f"\nâŒ ç›´è¿å¤±è´¥")
        print(f"é”™è¯¯ç±»å‹: {type(e).__name__}")
        print(f"é”™è¯¯ä¿¡æ¯: {str(e)}")
        return False

async def main():
    print("\n" + "ğŸ” å¼€å§‹æµ‹è¯• API é…ç½®".center(60, "="))
    
    try:
        # åŠ è½½é…ç½®
        llm_config = load_config()
        print("\nâœ“ é…ç½®æ–‡ä»¶åŠ è½½æˆåŠŸ")
        
        # æµ‹è¯•1: ä½¿ç”¨ä»£ç†
        result_with_proxy = await test_api_with_proxy(llm_config)
        
        # ç­‰å¾…ä¸€ä¸‹ï¼Œé¿å…è¯·æ±‚è¿‡å¿«
        await asyncio.sleep(1)
        
        # æµ‹è¯•2: ä¸ä½¿ç”¨ä»£ç†
        result_without_proxy = await test_api_without_proxy(llm_config)
        
        # æ€»ç»“
        print("\n" + "="*60)
        print("ğŸ“Š æµ‹è¯•ç»“æœæ€»ç»“")
        print("="*60)
        print(f"ä½¿ç”¨ä»£ç† (127.0.0.1:7890): {'âœ… å¯ç”¨' if result_with_proxy else 'âŒ ä¸å¯ç”¨'}")
        print(f"ç›´è¿ (æ— ä»£ç†):            {'âœ… å¯ç”¨' if result_without_proxy else 'âŒ ä¸å¯ç”¨'}")
        
        print("\nğŸ’¡ å»ºè®®:")
        if result_without_proxy and not result_with_proxy:
            print("   â†’ å»ºè®®åœ¨ AutoOpt.py ä¸­æ³¨é‡Šæ‰ä»£ç†è®¾ç½®ï¼Œä½¿ç”¨ç›´è¿")
            print("   â†’ ä¿®æ”¹ AutoOpt.py ç¬¬3-4è¡Œï¼Œåœ¨å‰é¢åŠ  # æ³¨é‡Šæ‰")
        elif result_with_proxy and not result_without_proxy:
            print("   â†’ APIéœ€è¦é€šè¿‡ä»£ç†è®¿é—®ï¼Œè¯·ç¡®ä¿ä»£ç†æœåŠ¡æ­£åœ¨è¿è¡Œ")
        elif result_with_proxy and result_without_proxy:
            print("   â†’ APIåœ¨æœ‰ä»£ç†å’Œæ— ä»£ç†æƒ…å†µä¸‹éƒ½å¯ç”¨")
            print("   â†’ å¦‚æœæ˜¯å›½å†…è®¿é—®é˜¿é‡Œäº‘APIï¼Œå»ºè®®ä½¿ç”¨ç›´è¿ï¼ˆä¸ä½¿ç”¨ä»£ç†ï¼‰")
        else:
            print("   â†’ APIé…ç½®å¯èƒ½æœ‰é—®é¢˜ï¼Œè¯·æ£€æŸ¥ï¼š")
            print("     1. API Keyæ˜¯å¦æœ‰æ•ˆ")
            print("     2. æ¨¡å‹åç§°æ˜¯å¦æ­£ç¡®")
            print("     3. Base URLæ˜¯å¦æ­£ç¡®")
            print("     4. ç½‘ç»œè¿æ¥æ˜¯å¦æ­£å¸¸")
        
        print("\n" + "="*60 + "\n")
        
    except FileNotFoundError:
        print("\nâŒ é”™è¯¯: æ‰¾ä¸åˆ°é…ç½®æ–‡ä»¶ config/config2.yaml")
    except Exception as e:
        print(f"\nâŒ å‘ç”Ÿé”™è¯¯: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    asyncio.run(main())

