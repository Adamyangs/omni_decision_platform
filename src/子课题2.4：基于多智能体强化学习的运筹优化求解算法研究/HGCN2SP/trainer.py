import os
import sys
import time
import torch
import numpy as np
import wandb
import pickle
import torch.nn as nn  
from agent import Agent
from env import CFLPEnv
from sample import Sampler
from utils import solve_cflp_softmax

class PPOTrainer:
    def __init__(self, args, policy_param, train_param, data, bs, clusters, run_name, device):
        self.args = args
        self.train_param = train_param
        self.run_name = run_name
        self.device = device
        self.envs = CFLPEnv(data, bs, clusters, train_param['sel_num'], args.num_envs, device=self.device)
        self.agent = Agent(policy_param, train_param, self.device).to(self.device)
        self.optimizer = torch.optim.Adam(self.agent.parameters(), lr=args.learning_rate, eps=1e-5)
        self.sampler = Sampler(self.envs, self.agent, args.num_steps, self.device)
        self.best_eval_delta = 100.0
        self.eval_epoch = 0
        self.use_wandb = args.track and hasattr(wandb, 'run') and wandb.run is not None
        
        # åˆ¤æ–­æ˜¯å¦åœ¨ç»ˆç«¯ä¸­è¿è¡Œï¼ˆnohupæ—¶ä¼šè¿”å›Falseï¼‰
        self.is_terminal = sys.stdout.isatty()
        
        # å¯¼å…¥tqdmå¹¶æ ¹æ®è¿è¡Œç¯å¢ƒè°ƒæ•´
        from tqdm import tqdm
        self.tqdm = tqdm
        
        # è®¾ç½®æ—¥å¿—çº§åˆ«
        self.log_level = args.log_level if hasattr(args, 'log_level') else 'INFO'
        
        # åˆå§‹åŒ–æ—¥å¿—è®°å½•å™¨
        self._init_logger()

    def _init_logger(self):
        """åˆå§‹åŒ–æ—¥å¿—ç³»ç»Ÿ"""
        import logging
        
        # åˆ›å»ºlogger
        self.logger = logging.getLogger(self.run_name)
        self.logger.setLevel(getattr(logging, self.log_level))
        
        # é¿å…é‡å¤æ·»åŠ handler
        if not self.logger.handlers:
            # åˆ›å»ºformatter
            formatter = logging.Formatter(
                '%(asctime)s - %(name)s - %(levelname)s - %(message)s',
                datefmt='%Y-%m-%d %H:%M:%S'
            )
            
            # æ§åˆ¶å°handlerï¼ˆä»…åœ¨ç»ˆç«¯è¿è¡Œæ—¶æ˜¾ç¤ºï¼‰
            if self.is_terminal:
                console_handler = logging.StreamHandler(sys.stdout)
                console_handler.setLevel(logging.INFO)
                console_handler.setFormatter(formatter)
                self.logger.addHandler(console_handler)
            
            # æ–‡ä»¶handlerï¼ˆå§‹ç»ˆè®°å½•åˆ°æ–‡ä»¶ï¼‰
            os.makedirs('logs', exist_ok=True)
            file_handler = logging.FileHandler(f'logs/{self.run_name}.log')
            file_handler.setLevel(logging.INFO)
            file_handler.setFormatter(formatter)
            self.logger.addHandler(file_handler)
    
    def log(self, message, level='INFO'):
        """ç»Ÿä¸€çš„æ—¥å¿—è®°å½•æ–¹æ³•"""
        log_method = getattr(self.logger, level.lower(), self.logger.info)
        log_method(message)
        
        # åŒæ—¶æ‰“å°åˆ°æ§åˆ¶å°ï¼ˆå¦‚æœä¸åœ¨ç»ˆç«¯ä¸­ï¼Œä½¿ç”¨ç®€å•çš„printï¼‰
        if not self.is_terminal and level in ['INFO', 'WARNING', 'ERROR']:
            print(f"{time.strftime('%Y-%m-%d %H:%M:%S')} - {level} - {message}")

    def train(self):
        next_obs, _ = self.envs.reset(seed=self.args.seed)
        next_done = torch.zeros(self.envs.batch_size, device=self.device)
        
        self.log(f"ğŸš€ Starting training with {self.args.num_iterations} iterations", "INFO")
        
        # åˆ›å»ºiterationè¿›åº¦æ¡ï¼ˆéç»ˆç«¯ç¯å¢ƒä¸‹ä½¿ç”¨ç®€å•æ ¼å¼ï¼‰
        if self.is_terminal:
            pbar_iter = self.tqdm(range(1, self.args.num_iterations + 1), 
                             desc="ğŸš€ Training", 
                             bar_format="{l_bar}{bar}| {n_fmt}/{total_fmt} iters [{elapsed}<{remaining}]",
                             colour='green')
        else:
            # éç»ˆç«¯ç¯å¢ƒä¸‹ä½¿ç”¨æ›´ç®€å•çš„æ ¼å¼ï¼Œé¿å…æ§åˆ¶å­—ç¬¦
            pbar_iter = self.tqdm(range(1, self.args.num_iterations + 1), 
                             desc="Training", 
                             bar_format="{l_bar}{bar}| {n_fmt}/{total_fmt} iters",
                             ascii=True,  # ä½¿ç”¨ASCIIå­—ç¬¦ï¼Œé¿å…ä¹±ç 
                             mininterval=5.0,  # å‡å°‘æ›´æ–°é¢‘ç‡
                             maxinterval=10.0)

        start_time = time.time()
        
        for iteration in pbar_iter:
            if self.args.anneal_lr:
                self._anneal_learning_rate(iteration)

            # æ•°æ®é‡‡é›†
            next_obs, next_done = self.sampler.collect_trajectories(next_obs)

            # è®¡ç®—ä¼˜åŠ¿ä¸å›æŠ¥
            advantages, returns = self.sampler.compute_advantages_and_returns(self.args)

            # è·å–æ‰¹æ•°æ®
            b_obs, b_actions, b_logprobs, b_advantages, b_returns, b_values = self.sampler.get_batch_data(advantages, returns)

            # ç­–ç•¥ä¼˜åŒ–
            avg_loss = self._update_policy(b_obs, b_actions, b_logprobs, b_advantages, b_returns, b_values)

            # è¯„ä¼°ä¸ä¿å­˜æ¨¡å‹
            if iteration % self.train_param['eval_epoch'] == 0:
                self._evaluate_and_save(iteration, pbar_iter)
            
            # æ›´æ–°è¿›åº¦æ¡ä¿¡æ¯
            pbar_iter.set_postfix({
                'lr': f'{self.optimizer.param_groups[0]["lr"]:.2e}',
                'loss': f'{avg_loss:.4f}'
            })
            
            # å®šæœŸè®°å½•æ—¥å¿—
            if iteration % 10 == 0:
                self.log(f"Iteration {iteration}: loss={avg_loss:.4f}, lr={self.optimizer.param_groups[0]['lr']:.2e}")
        
        # è®­ç»ƒå®Œæˆåä¿å­˜æ¨¡å‹ 
        if self.args.save_model:
            model_path = f"runs/{self.run_name}/{self.args.exp_name}.pt"
            folder_path = os.path.dirname(model_path)
            os.makedirs(folder_path, exist_ok=True)
            torch.save(self.agent.state_dict(), model_path)
            self.log(f"ğŸ’¾ Model saved to {model_path}")
        
        # å…³é—­è¿›åº¦æ¡
        pbar_iter.close()
        
        # æ‰“å°è®­ç»ƒå®Œæˆä¿¡æ¯
        total_time = time.time() - start_time
        self.log("\n" + "="*60, "INFO")
        self.log("ğŸ‰ Training Completed!", "INFO")
        self.log(f"ğŸ“Š Total iterations: {self.args.num_iterations}", "INFO")
        self.log(f"â±ï¸  Total time: {total_time:.2f}s ({total_time/60:.2f}min)", "INFO")
        self.log(f"ğŸ“ˆ Best eval delta: {self.best_eval_delta:.2f}%", "INFO")
        self.log("="*60, "INFO")
        
        self.envs.close()
    
    def _update_policy(self, b_obs, b_actions, b_logprobs, b_advantages, b_returns, b_values):
        # Optimizing the policy and value network
        b_inds = np.arange(self.args.batch_size)
        clipfracs = []
        
        # è®¡ç®—æ€»è¿­ä»£æ¬¡æ•°
        total_updates = self.args.update_epochs * (self.args.batch_size // self.args.minibatch_size)
        if self.args.batch_size % self.args.minibatch_size != 0:
            total_updates += self.args.update_epochs
        
        # åˆ›å»ºç­–ç•¥æ›´æ–°è¿›åº¦æ¡ï¼ˆéç»ˆç«¯ç¯å¢ƒä¸‹ç®€åŒ–ï¼‰
        if self.is_terminal:
            pbar_update = self.tqdm(total=total_updates, 
                                  desc="ğŸ”„ Updating", 
                                  bar_format="{l_bar}{bar}| {n_fmt}/{total_fmt} updates",
                                  colour='cyan',
                                  leave=False)
        else:
            # éç»ˆç«¯ç¯å¢ƒä¸‹ä½¿ç”¨ç®€å•æ ¼å¼
            pbar_update = self.tqdm(total=total_updates, 
                                  desc="Updating", 
                                  bar_format="{l_bar}{bar}| {n_fmt}/{total_fmt}",
                                  ascii=True,
                                  leave=True,  # ä¿ç•™åœ¨æ–‡ä»¶ä¸­
                                  mininterval=2.0)
        
        avg_loss = 0
        update_count = 0
        last_log_time = time.time()
        
        for epoch in range(self.args.update_epochs):
            np.random.shuffle(b_inds)
            for start in range(0, self.args.batch_size, self.args.minibatch_size):
                end = start + self.args.minibatch_size
                mb_inds = b_inds[start:end]

                b_obs_m = [b_obs[i] for i in mb_inds]

                _, newlogprob, entropy, newvalue = self.agent.get_action_and_value(b_obs_m, b_actions[mb_inds])
                logratio = newlogprob - b_logprobs[mb_inds]
                ratio = logratio.exp()

                with torch.no_grad():
                    approx_kl = ((ratio - 1) - logratio).mean()
                    clipfrac = ((ratio - 1.0).abs() > self.args.clip_coef).float().mean().item()
                    clipfracs.append(clipfrac)

                mb_advantages = b_advantages[mb_inds]
                if self.args.norm_adv:
                    mb_advantages = (mb_advantages - mb_advantages.mean()) / (mb_advantages.std() + 1e-8)

                # Policy loss
                pg_loss1 = -mb_advantages * ratio
                pg_loss2 = -mb_advantages * torch.clamp(ratio, 1 - self.args.clip_coef, 1 + self.args.clip_coef)
                pg_loss = torch.max(pg_loss1, pg_loss2).mean()

                # Value loss
                newvalue = newvalue.view(-1)
                if self.args.clip_vloss:
                    v_loss_unclipped = (newvalue - b_returns[mb_inds]) ** 2
                    v_clipped = b_values[mb_inds] + torch.clamp(
                        newvalue - b_values[mb_inds],
                        -self.args.clip_coef,
                        self.args.clip_coef,
                    )
                    v_loss_clipped = (v_clipped - b_returns[mb_inds]) ** 2
                    v_loss_max = torch.max(v_loss_unclipped, v_loss_clipped)
                    v_loss = 0.5 * v_loss_max.mean()
                else:
                    v_loss = 0.5 * ((newvalue - b_returns[mb_inds]) ** 2).mean()

                entropy_loss = entropy.mean()
                loss = pg_loss - self.args.ent_coef * entropy_loss + v_loss * self.args.vf_coef

                # æ›´æ–°å¹³å‡æŸå¤±
                avg_loss += loss.item()
                update_count += 1
                
                # æ›´æ–°è¿›åº¦æ¡ä¿¡æ¯
                if self.is_terminal or time.time() - last_log_time > 2.0:
                    pbar_update.set_postfix({
                        'loss': f'{loss.item():.4f}',
                        'kl': f'{approx_kl.item():.4f}'
                    })
                    last_log_time = time.time()
                
                # æ›´æ–°è¿›åº¦æ¡
                pbar_update.update(1)

                if self.use_wandb:
                    wandb.log({
                        'update/loss': loss.item(),
                        'update/pg_loss': pg_loss.item(),
                        'update/v_loss': v_loss.item(),
                        'update/entropy': entropy_loss.item(),
                        'update/clipfrac': clipfrac,
                        'update/approx_kl': approx_kl.item()
                    })
                
                self.optimizer.zero_grad()
                loss.backward()
                nn.utils.clip_grad_norm_(self.agent.parameters(), self.args.max_grad_norm)
                self.optimizer.step()

            if self.args.target_kl is not None and approx_kl > self.args.target_kl:
                self.log(f"ğŸ›‘ Early stopping at epoch {epoch+1} (KL threshold reached)")
                break
        
        # å…³é—­ç­–ç•¥æ›´æ–°è¿›åº¦æ¡
        pbar_update.close()
        
        # è®¡ç®—å¹³å‡æŸå¤±
        avg_loss = avg_loss / update_count if update_count > 0 else 0
        
        return avg_loss

    def _anneal_learning_rate(self, iteration):
        frac = 1.0 - (iteration - 1.0) / self.args.num_iterations
        self.optimizer.param_groups[0]["lr"] = frac * self.args.learning_rate
    
    def eval_model(self, eval_cls_path, action, train_param):
        mean_bs = 0
        mean_agent = 0
        mean_time = 0
        delta = 0
        eval_num = min(100, len(eval_cls_path))
        
        self.log(f"ğŸ“Š Evaluating {eval_num} samples")
        
        # åˆ›å»ºevalè¿›åº¦æ¡
        if self.is_terminal:
            pbar_eval = self.tqdm(range(eval_num), 
                               desc="ğŸ“Š Evaluating", 
                               bar_format="{l_bar}{bar}| {n_fmt}/{total_fmt} samples",
                               colour='yellow',
                               leave=False)
        else:
            pbar_eval = self.tqdm(range(eval_num), 
                               desc="Evaluating", 
                               bar_format="{l_bar}{bar}| {n_fmt}/{total_fmt}",
                               ascii=True,
                               leave=True)
        
        results = []
        
        for i in pbar_eval:
            cls_loc = os.path.join(train_param["eval_cls_loc"], eval_cls_path[i]) 
            with open(cls_loc, 'rb') as f:
                cls = pickle.load(f)
            file_path = f"result_of_{eval_cls_path[i][10:-4]}.pkl"
            file_path = os.path.join(train_param["eval_result"], file_path)
            with open(file_path, "rb") as f:
                now_result = pickle.load(f)
                bs = now_result['primal']
                mean_bs += bs
            args = (cls, action[i].cpu(), True)
            eval_results = solve_cflp_softmax(args).squeeze()
            
            # æ›´æ–°è¿›åº¦æ¡ä¿¡æ¯
            if self.is_terminal:
                pbar_eval.set_postfix({
                    'agent': f'{eval_results[0].item():.2f}',
                    'gap': f'{(eval_results[0].item() - bs)/ bs *100:.2f}%'
                })
            
            gap = (eval_results[0].item() - bs)/ bs *100
            results.append({
                'sample': i+1,
                'agent': eval_results[0].item(),
                'bs': bs,
                'gap': gap,
                'time': eval_results[1].item()
            })
            
            mean_agent += eval_results[0].item()
            mean_time += eval_results[1].item()
            delta += gap

        # å…³é—­evalè¿›åº¦æ¡
        pbar_eval.close()
        
        mean_agent = mean_agent / eval_num
        mean_bs = mean_bs / eval_num
        mean_time = mean_time / eval_num
        delta /= eval_num
        
        # è®°å½•è¯„ä¼°ç»“æœ
        self.log("\n" + "="*60, "INFO")
        self.log("ğŸ“Š EVALUATION SUMMARY", "INFO")
        self.log(f"  Average agent result: {mean_agent:.2f}", "INFO")
        self.log(f"  Average baseline: {mean_bs:.2f}", "INFO")
        self.log(f"  Average time: {mean_time:.2f}s", "INFO")
        self.log(f"  Average gap with Gurobi: {delta:.2f}%", "INFO")
        self.log("="*60, "INFO")
        
        # ä¿å­˜è¯¦ç»†ç»“æœ
        os.makedirs('eval_results', exist_ok=True)
        result_file = f'eval_results/{self.run_name}_eval_{self.eval_epoch}.txt'
        with open(result_file, 'w') as f:
            f.write(f"Evaluation at epoch {self.eval_epoch}\n")
            f.write(f"Average agent: {mean_agent:.2f}\n")
            f.write(f"Average baseline: {mean_bs:.2f}\n")
            f.write(f"Average gap: {delta:.2f}%\n")
            f.write("\nDetailed results:\n")
            for r in results:
                f.write(f"Sample {r['sample']}: agent={r['agent']:.2f}, bs={r['bs']:.2f}, gap={r['gap']:.2f}%, time={r['time']:.2f}s\n")
        
        if delta < self.best_eval_delta:
            self.log(f"ğŸ’¾ Saving best model (new best delta: {delta:.2f}% < {self.best_eval_delta:.2f}%)")
            self.best_eval_delta = delta
            model_path = f"./model_path/{self.run_name}_eval_{self.eval_epoch}_{delta:.2f}.pt"
            torch.save(self.agent.state_dict(), model_path)
            self.log(f"âœ… Model saved to {model_path}")
        
        self.eval_epoch += 1
        
        if self.use_wandb:
            wandb.log({
                "eval/mean_agent": mean_agent, 
                "eval/mean_bs": mean_bs, 
                "eval/mean_time": mean_time, 
                "eval/delta": delta,
                "eval/best_delta": self.best_eval_delta
            })
        
        return delta
    
    def _evaluate_and_save(self, iteration, pbar_iter=None):
        train_param = self.train_param
        
        # ä¿å­˜å½“å‰æ¨¡å‹
        model_path = f"./model_path/{self.run_name}_seed_{self.args.seed}_{iteration}.pt"
        torch.save(self.agent.state_dict(), model_path)
        
        # åœ¨è¿›åº¦æ¡ä¸­æ˜¾ç¤ºè¯„ä¼°çŠ¶æ€
        if pbar_iter:
            pbar_iter.set_description(f"Evaluating (Iter {iteration})")
        
        self.log(f"\nğŸ” Starting evaluation at iteration {iteration}...")
        
        eval_data = torch.load(os.path.join(train_param['eval_path'], train_param['eval_pt']))   
        for i in range(len(eval_data)):
            eval_data[i] = eval_data[i].to(self.device)
            
        with open(os.path.join(train_param['eval_path'], train_param['eval_cls']), 'rb') as f:   
            eval_cls_path = pickle.load(f)
            
        with torch.no_grad():
            action, _, _, _ = self.agent.get_action_and_value(eval_data, decode_type="greedy")
            delta = self.eval_model(eval_cls_path, action, train_param)
        
        # æ¢å¤è®­ç»ƒè¿›åº¦æ¡æè¿°
        if pbar_iter:
            pbar_iter.set_description("Training")
            
        self.log(f"âœ… Evaluation completed at iteration {iteration}, average gap: {delta:.2f}%\n")